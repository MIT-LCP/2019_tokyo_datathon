{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-prediction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MIT-LCP/2019_tokyo_datathon/blob/master/04_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T3wdKZCPklNq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# eICU Collaborative Research Database\n",
        "\n",
        "# Notebook 4: Prediction\n",
        "\n",
        "This notebook explores how a decision trees can be trained to predict in-hospital mortality of patients.\n"
      ]
    },
    {
      "metadata": {
        "id": "rG3HrM7GkwCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load libraries and connect to the database"
      ]
    },
    {
      "metadata": {
        "id": "s-MoFA6NkkbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# model building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn import impute\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Make pandas dataframes prettier\n",
        "from IPython.display import display, HTML, Image\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Access data using Google BigQuery.\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyBV_Q9DkyD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# authenticate\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cF1udJKhkzYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up environment variables\n",
        "project_id='datathonjapan2019'\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]=project_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGurBAQIUDTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To make our lives easier, we'll also install and import a set of helper functions from the `datathon2` package. We will be using the following functions from the package:\n",
        "- `plot_model_pred_2d`: to visualize our data, helping to display a class split assigned by a tree vs the true class.\n",
        "- `run_query()`: to run an SQL query against our BigQuery database and assign the results to a dataframe. \n"
      ]
    },
    {
      "metadata": {
        "id": "GDEewAlvk0oT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install datathon2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JM6O5GPAUI89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datathon2 as dtn\n",
        "import pydotplus\n",
        "from tableone import TableOne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hq_09Hh-y17k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we'll be looking at tree models, so we'll now install a package for visualizing these models."
      ]
    },
    {
      "metadata": {
        "id": "jBMOwgwszGOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install graphviz -y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LgcRCqxCk3HC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the patient cohort\n",
        "\n",
        "Let's extract a cohort of patients admitted to the ICU from the emergency department.  We link demographics data from the `patient` table to severity of illness score data in the `apachepatientresult` table. We exclude readmissions and neurological patients to help create a population suitable for our demonstration."
      ]
    },
    {
      "metadata": {
        "id": "ReCl7-aek1-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Link the patient, apachepatientresult, and apacheapsvar tables on patientunitstayid\n",
        "# using an inner join.\n",
        "query = \"\"\"\n",
        "SELECT p.unitadmitsource, p.gender, p.age, p.unittype, p.unitstaytype, \n",
        "    a.actualhospitalmortality, a.acutePhysiologyScore, a.apacheScore\n",
        "FROM `physionet-data.eicu_crd_demo.patient` p\n",
        "INNER JOIN `physionet-data.eicu_crd_demo.apachepatientresult` a\n",
        "ON p.patientunitstayid = a.patientunitstayid\n",
        "WHERE a.apacheversion LIKE 'IVa'\n",
        "AND LOWER(p.unitadmitsource) LIKE \"%emergency%\"\n",
        "AND LOWER(p.unitstaytype) LIKE \"admit%\"\n",
        "AND LOWER(p.unittype) NOT LIKE \"%neuro%\";\n",
        "\"\"\"\n",
        "\n",
        "cohort = dtn.run_query(query,project_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yxLctVBpk9sO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cohort.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NPlwRV2buYb1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data for analysis\n",
        "\n",
        "Before continuing, we want to review our data, paying attention to factors such as:\n",
        "- data types (for example, are values recorded as characters or numerical values?) \n",
        "- missing data\n",
        "- distribution of values"
      ]
    },
    {
      "metadata": {
        "id": "v3OJ4LDvueKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dataset info\n",
        "print(cohort.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s4wQ6o_RvLph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Encode the categorical data\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "cohort['gender_code'] = encoder.fit_transform(cohort['gender'])\n",
        "cohort['actualhospitalmortality_code'] = encoder.fit_transform(cohort['actualhospitalmortality'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ogi_ns-ylnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Handle the deidentified ages\n",
        "cohort['age'] = pd.to_numeric(cohort['age'], downcast='integer', errors='coerce')\n",
        "cohort['age'] = cohort['age'].fillna(value=91.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77M0QJQ5wcPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preview the encoded data\n",
        "cohort[['gender','gender_code']].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqvwTNPN3KZz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the outcome variable\n",
        "cohort['actualhospitalmortality_code'].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIIsthy1WK3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# View summary statistics\n",
        "pd.set_option('display.height', 500)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "TableOne(cohort,groupby='actualhospitalmortality')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IGtKlTG1gvRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From these summary statistics, we can see that the average age is higher in the group of patients who do not survive. What other differences do you see?"
      ]
    },
    {
      "metadata": {
        "id": "ze7y5J4Ioz8u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create our train and test sets\n",
        "\n",
        "We only focus on two variables for our analysis, age and acute physiology score. Limiting ourselves to two variables will make it easier to visualize our models."
      ]
    },
    {
      "metadata": {
        "id": "i5zXkn_AlDJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = ['age','acutePhysiologyScore']\n",
        "outcome = 'actualhospitalmortality_code'\n",
        "\n",
        "X = cohort[features]\n",
        "y = cohort[outcome]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IHhIgDUwocmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvQWkuY6nkZ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Review the number of cases in each set\n",
        "print(\"Train data: {}\".format(len(X_train)))\n",
        "print(\"Test data: {}\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2waK5qBqanC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decision trees\n",
        "\n",
        "Let's build the simplest tree model we can think of: a classification tree with only one split. Decision trees of this form are commonly referred to under the umbrella term Classification and Regression Trees (CART) [1]. \n",
        "\n",
        "While we will only be looking at classification here, regression isn't too different. After grouping the data (which is essentially what a decision tree does), classification involves assigning all members of the group to the majority class of that group during training. Regression is the same, except you would assign the average value, not the majority. \n",
        "\n",
        "In the case of a decision tree with one split, often called a \"stump\", the model will partition the data into two groups, and assign classes for those two groups based on majority vote. There are many parameters available for the DecisionTreeClassifier class; by specifying max_depth=1 we will build a decision tree with only one split - i.e. of depth 1.\n",
        "\n",
        "[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."
      ]
    },
    {
      "metadata": {
        "id": "RlG3N3OYBqAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# specify max_depth=1 so we train a stump, i.e. a tree with only 1 split\n",
        "mdl = tree.DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# fit the model to the data - trying to predict y from X\n",
        "mdl = mdl.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RlioUw8B_0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model is so simple that we can look at the full decision tree."
      ]
    },
    {
      "metadata": {
        "id": "G2t9Nz8pBqEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graph = dtn.create_graph(mdl,feature_names=features)\n",
        "Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-iPwWWKCGY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we see three nodes: a node at the top, a node in the lower left, and a node in the lower right.\n",
        "\n",
        "The top node is the root of the tree: it contains all the data. Let's read this node bottom to top:\n",
        "- `value = [384, 44]`:  Current class balance. There are 384 observations of class 0 and 44 observations of class 1.\n",
        "- `samples = 428`:  Number of samples assessed at this node.\n",
        "- `gini = 0.184`: Gini impurity, a measure of \"impurity\". The higher the value, the bigger the mix of classes. A 50/50 split of two classes would result in an index of 0.5.\n",
        "- `acutePhysiologyScore <=78.5`: Decision rule learned by the node. In this case, patients with a score of <= 78.5 are moved into the left node and >78.5 to the right. "
      ]
    },
    {
      "metadata": {
        "id": "KS0UcZqUeJKz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The gini impurity is is actually used by the algorithm to determine a split. The model evaluates every feature (in our case, age and score) at every possible split (46, 47, 48..) to find the point with the lowest gini impurity in two resulting nodes. \n",
        "\n",
        "The approach is referred to as \"greedy\" because we are choosing the optimal split given our current state. Let's take a closer look at our decision boundary."
      ]
    },
    {
      "metadata": {
        "id": "uXl22sNTtpHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at the regions in a 2d plot\n",
        "# based on scikit-learn tutorial plot_iris.html\n",
        "plt.figure(figsize=[10,8])\n",
        "dtn.plot_model_pred_2d(mdl, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "25zSX-inCNOJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this plot we can see the decision boundary on the y-axis, separating the predicted classes. The true classes are indicated at each point. Where the background and point colours are mismatched, there has been misclassification. Of course we are using a very simple model. Let's see what happens when we increase the depth."
      ]
    },
    {
      "metadata": {
        "id": "ZuO62CL3CSGm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = tree.DecisionTreeClassifier(max_depth=5)\n",
        "mdl = mdl.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A88Vi83LCSJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[10,8])\n",
        "dtn.plot_model_pred_2d(mdl, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B88XlKDtCYmn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now our tree is more complicated! We can see a few vertical boundaries as well as the horizontal one from before. Some of these we may like, but some appear unnatural. Let's look at the tree itself."
      ]
    },
    {
      "metadata": {
        "id": "V1VLrOJJCcWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graph = dtn.create_graph(mdl,feature_names=features)\n",
        "Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ton_EnvFqHIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at the tree, we can see that there are some very specific rules. Consider our patient aged 65 years with an acute physiology score of 87. From the top of the tree, we would work our way down:\n",
        "\n",
        "- acutePhysiologyScore <= 78.5? No.\n",
        "- acutePhysiologyScore <= 106.5? Yes.\n",
        "- age <= 75.5? Yes\n",
        "- age <= 66. Yes.\n",
        "- age <= 62.5? No. \n",
        "\n",
        "This leads us to our single node with a gini impurity of 0. Having an entire rule based upon this one observation seems silly, but it is perfectly logical as at the moment. The only objective the algorithm cares about is minimizing the gini impurity. \n",
        "\n",
        "We are at risk of overfitting our data! This is where \"pruning\" comes in."
      ]
    },
    {
      "metadata": {
        "id": "VvsNIjCDDIo_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# let's prune the model and look again\n",
        "dtn.prune(mdl, min_samples_leaf = 10)\n",
        "graph = dtn.create_graph(mdl,feature_names=features)\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pRzzV2VvdxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above, we can see that our second tree is (1) smaller in depth, and (2) never splits a node with <= 10 samples. We can look at the decision surface for this tree:"
      ]
    },
    {
      "metadata": {
        "id": "5LyGDz-Cr-mU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[10,8])\n",
        "dtn.plot_model_pred_2d(mdl, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAnqmD_Dv_dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our pruned decision tree has a much more intuitive boundary, but does make some errors. We have reduced our performance in an effort to simplify the tree. This is the classic machine learning problem of trading off complexity with error.\n",
        "\n",
        "Note that, in order to do this, we \"invented\" the minimum samples per leaf node of 10. Why 10? Why not 5? Why not 20? The answer is: it depends on the dataset. Heuristically choosing these parameters can be time consuming, and we will see later on how gradient boosting elegantly handles this task."
      ]
    },
    {
      "metadata": {
        "id": "m54-38_gwD1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "2EFINpj-wD7H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decision trees have high \"variance\"\n",
        "\n",
        "Before we move on to boosting, it will be useful to demonstrate how decision trees have high \"variance\". In this context, variance refers to a property of some models to have a wide range of performance given random samples of data. Let's take a look at randomly slicing the data we have too see what that means."
      ]
    },
    {
      "metadata": {
        "id": "JT7fuuj6vjKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "fig = plt.figure(figsize=[12,3])\n",
        "\n",
        "for i in range(3):\n",
        "    ax = fig.add_subplot(1,3,i+1)\n",
        "\n",
        "    # generate indices in a random order\n",
        "    idx = np.random.permutation(X_train.shape[0])\n",
        "    \n",
        "    # only use the first 50\n",
        "    idx = idx[:50]\n",
        "    X_temp = X_train.iloc[idx]\n",
        "    y_temp = y_train.values[idx]\n",
        "    \n",
        "    # initialize the model\n",
        "    mdl = tree.DecisionTreeClassifier(max_depth=5)\n",
        "    \n",
        "    # train the model using the dataset\n",
        "    mdl = mdl.fit(X_temp, y_temp)\n",
        "    dtn.plot_model_pred_2d(mdl, X_temp, y_temp, cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6VTIDr-yRRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above we can see that we are using random subsets of data, and as a result, our decision boundary can change quite a bit. As you could guess, we actually don't want a model that randomly works well and randomly works poorly, so you may wonder why this is useful. \n",
        "\n",
        "The trick is that by combining many of instances of \"high variance\" classifiers (decision trees), we can end up with a single classifier with low variance. There is an old joke: two farmers and a statistician go hunting. They see a deer: the first farmer shoots, and misses to the left. The next farmer shoots, and misses to the right. The statistician yells \"We got it!!\".\n",
        "\n",
        "While it doesn't quite hold in real life, it turns out that this principle does hold for decision trees. Combining them in the right way ends up building powerful models."
      ]
    },
    {
      "metadata": {
        "id": "iWnKvx6myf9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "The premise of boosting is the combination of many weak learners to form a single \"strong\" learner. In a nutshell, boosting involves building a models iteratively, and at each step we focus on the data we performed poorly on. In our context, we'll use decision trees, so the first step would be to build a tree using the data. Next, we'd look at the data that we misclassified, and re-weight the data so that we really wanted to classify those observations correctly, at a cost of maybe getting some of the other data wrong this time. Let's see how this works in practice."
      ]
    },
    {
      "metadata": {
        "id": "YJWxu0bTwRzD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
        "mdl = ensemble.AdaBoostClassifier(base_estimator=clf,n_estimators=6)\n",
        "mdl = mdl.fit(X_train,y_train)\n",
        "\n",
        "# plot each individual decision tree\n",
        "fig = plt.figure(figsize=[10,5])\n",
        "for i, estimator in enumerate(mdl.estimators_):\n",
        "    ax = fig.add_subplot(2,3,i+1)\n",
        "    dtn.plot_model_pred_2d(estimator, X_train, y_train, cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zNfvDjTzh2U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at the above, we can see that the first iteration builds the exact same simple decision tree as we had seen earlier. This makes sense - it's using the entire dataset with no special weighting. \n",
        "\n",
        "In the next iteration we can see the model shift - it misclassified five observations in class 1, and now these are the most important observations. Consequently, it picks the boundary that, while prioritizing correctly classifies these observations, still tries to best classify the rest of the data too. Now we have correctly classified all but one observation, the one on the far left middle of the graph. In iteration 3, the algorithm solely focuses on correctly classifying this one observation.\n",
        "\n",
        "One important point is that each tree is weighted by it's global error. In the figure above, it's obvious that we wouldn't want to weight Tree 3 equally to Tree 1, when Tree 1 is doing so much better overall. It turns out that weighting each tree by the inverse of its error is a pretty good way to do this.\n",
        "\n",
        "Let's look at final model's decision surface.\n"
      ]
    },
    {
      "metadata": {
        "id": "3pVG5ytfzp_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot the final prediction\n",
        "plt.figure(figsize=[9,5])\n",
        "dtn.plot_model_pred_2d(mdl, X_train, y_train)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRGRFjRgz26h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And that's AdaBoost! There are a few tricks we have glossed over here - but you understand the general principle. Now we'll move on to a different approach. With boosting, we iteratively changed the dataset to have new trees focus on the \"difficult\" observations. The next approach we discuss is similar as it also involves using changed versions of our dataset to build new trees."
      ]
    },
    {
      "metadata": {
        "id": "JrXAspvrzv8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}